{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyian/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assignment Owner: Tian Wang\n",
    "\n",
    "#######################################\n",
    "#### Normalization\n",
    "\n",
    "\n",
    "def feature_normalization(train, test):\n",
    "    \"\"\"Rescale the data so that each feature in the training set is in\n",
    "    the interval [0,1], and apply the same transformations to the test\n",
    "    set, using the statistics computed on the training set.\n",
    "\n",
    "    Args:\n",
    "        train - training set, a 2D numpy array of size (num_instances, num_features)\n",
    "        test  - test set, a 2D numpy array of size (num_instances, num_features)\n",
    "    Returns:\n",
    "        train_normalized - training set after normalization\n",
    "        test_normalized  - test set after normalization\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    train_normalized = np.zeros((train.shape[0],train.shape[1]))\n",
    "    test_normalized = np.zeros((test.shape[0],test.shape[1]))\n",
    "    for idx in range(train.shape[1]):\n",
    "        mean_val = np.mean(train[:,idx], axis = 0)\n",
    "        max_val = np.max(train[:,idx], axis = 0)\n",
    "        min_val = np.min(train[:,idx], axis = 0)\n",
    "        \n",
    "        if max_val > 1 or min_val<0 and (max_val != min_val):\n",
    "            train_normalized[:,idx] = (train[:,idx] - min_val)/(max_val - min_val)\n",
    "            test_normalized[:,idx] = (test[:,idx] - min_val)/(max_val - min_val)\n",
    "        else:\n",
    "            train_normalized[:,idx] = train[:,idx]\n",
    "            test_normalized[:,idx] = test[:,idx]\n",
    "        \n",
    "    return (train_normalized, test_normalized)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', delimiter=',')\n",
    "X = df.values[:,:-1]\n",
    "y = df.values[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =100, random_state=10)\n",
    "\n",
    "X_train, X_test = feature_normalization(X_train, X_test)\n",
    "X_train = np.hstack((X_train, np.ones((X_train.shape[0], 1))))  # Add bias term\n",
    "X_test = np.hstack((X_test, np.ones((X_test.shape[0], 1)))) # Add bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "#### The square loss function\n",
    "\n",
    "def compute_square_loss(X, y, theta):\n",
    "    \"\"\"\n",
    "    Given a set of X, y, theta, compute the square loss for predicting y with X*theta\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D array of size (num_features)\n",
    "\n",
    "    Returns:\n",
    "        loss - the square loss, scalar\n",
    "    \"\"\"\n",
    "    loss = 0 #initialize the square_loss\n",
    "    #TODO\n",
    "    m = X.shape[0]\n",
    "    return 1/m*(np.dot(y.transpose(),y)-2*np.dot(np.dot(y.transpose(),X),theta)+\n",
    "               np.dot(theta.transpose(),np.dot(X.transpose(), np.dot(X, theta))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "### compute the gradient of square loss function\n",
    "def compute_square_loss_gradient(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute gradient of the square loss (as defined in compute_square_loss), at the point theta.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "\n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size (num_features)\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    m = X.shape[0]\n",
    "    return 2/m*(-np.dot(X.transpose(), y) + np.dot(np.dot(X.transpose(),X), theta))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "### Gradient Checker\n",
    "#Getting the gradient calculation correct is often the trickiest part\n",
    "#of any gradient-based optimization algorithm.  Fortunately, it's very\n",
    "#easy to check that the gradient calculation is correct using the\n",
    "#definition of gradient.\n",
    "#See http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization\n",
    "def grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4):\n",
    "    \"\"\"Implement Gradient Checker\n",
    "    Check that the function compute_square_loss_gradient returns the\n",
    "    correct gradient for the given X, y, and theta.\n",
    "\n",
    "    Let d be the number of features. Here we numerically estimate the\n",
    "    gradient by approximating the directional derivative in each of\n",
    "    the d coordinate directions:\n",
    "    (e_1 = (1,0,0,...,0), e_2 = (0,1,0,...,0), ..., e_d = (0,...,0,1)\n",
    "\n",
    "    The approximation for the directional derivative of J at the point\n",
    "    theta in the direction e_i is given by:\n",
    "    ( J(theta + epsilon * e_i) - J(theta - epsilon * e_i) ) / (2*epsilon).\n",
    "\n",
    "    We then look at the Euclidean distance between the gradient\n",
    "    computed using this approximation and the gradient computed by\n",
    "    compute_square_loss_gradient(X, y, theta).  If the Euclidean\n",
    "    distance exceeds tolerance, we say the gradient is incorrect.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "        epsilon - the epsilon used in approximation\n",
    "        tolerance - the tolerance error\n",
    "\n",
    "    Return:\n",
    "        A boolean value indicate whether the gradient is correct or not\n",
    "\n",
    "    \"\"\"\n",
    "    true_gradient = compute_square_loss_gradient(X, y, theta) #the true gradient\n",
    "    num_features = theta.shape[0]\n",
    "    approx_grad = np.zeros(num_features) #Initialize the gradient we approximate\n",
    "    #TODO\n",
    "    for i in range(num_features):\n",
    "        e_i = np.zeros(num_features)\n",
    "        e_i[i] = 1\n",
    "        approx_grad[i] = (compute_square_loss(X, y, theta+epsilon*e_i)-compute_square_loss(X, y, theta-epsilon*e_i))/(2*epsilon)\n",
    "    distance = np.linalg.norm(approx_grad-true_gradient.transpose())\n",
    "    if distance > tolerance:\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "### Generic Gradient Checker\n",
    "def generic_gradient_checker(X, y, theta, objective_func, gradient_func, epsilon=0.01, tolerance=1e-4):\n",
    "    \"\"\"\n",
    "    The functions takes objective_func and gradient_func as parameters. And check whether gradient_func(X, y, theta) returned\n",
    "    the true gradient for objective_func(X, y, theta).\n",
    "    Eg: In LSR, the objective_func = compute_square_loss, and gradient_func = compute_square_loss_gradient\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    true_gradient = gradient_func(X,y,theta)\n",
    "    num_features = theta.shape[0]\n",
    "    approx_grad = np.zeros(num_features) #Initialize the gradient we approximate\n",
    "    for i in range(num_features):\n",
    "        e_i = np.zeros(num_features)\n",
    "        e_i[i] = 1\n",
    "        approx_grad[i] = (objective_func(X, y, theta+epsilon*e_i)-objective_func(X, y, theta-epsilon*e_i))/(2*epsilon)\n",
    "    distance = np.linalg.norm(approx_grad-true_gradient.transpose())\n",
    "    if distance > tolerance:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "#### Batch Gradient Descent\n",
    "def batch_grad_descent(X, y, alpha=0.1, num_iter=1000, check_gradient=False):\n",
    "    \"\"\"\n",
    "    In this question you will implement batch gradient descent to\n",
    "    minimize the square loss objective\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        num_iter - number of iterations to run\n",
    "        check_gradient - a boolean value indicating whether checking the gradient when updating\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - store the the history of parameter vector in iteration, 2D numpy array of size (num_iter+1, num_features)\n",
    "                    for instance, theta in iteration 0 should be theta_hist[0], theta in ieration (num_iter) is theta_hist[-1]\n",
    "        loss_hist - the history of objective function vector, 1D numpy array of size (num_iter+1)\n",
    "    \"\"\"\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta_hist = np.zeros((num_iter+1, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_iter+1) #initialize loss_hist\n",
    "    theta = np.zeros(num_features) #initialize theta\n",
    "    #TODO\n",
    "    theta_hist[0] = theta\n",
    "    loss_hist[0] = compute_square_loss(X,y,theta)\n",
    "    for i in range(num_iter):\n",
    "        if check_gradient and (not grad_checker(X,y,theta_hist[i])):\n",
    "            return False\n",
    "        elif (not check_gradient) or (check_gradient and grad_checker(X,y,theta_hist[i])):\n",
    "            grad = compute_square_loss_gradient(X,y,theta_hist[i])\n",
    "            loss = compute_square_loss(X,y,theta_hist[i])\n",
    "            theta_hist[i+1] = theta_hist[i]-alpha*grad\n",
    "            loss_hist[i+1] = loss\n",
    "    if check_gradient and (not grad_checker(X,y,theta_hist[num_iter])):\n",
    "        return False\n",
    "    return (theta_hist, loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npZKEJnyllEoSmvCVUipJ/H87/TRFU0zNDgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11630ec18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "plot_mat = []\n",
    "legend_list = []\n",
    "alpha_list = [0.5, 0.1, 0.05, 0.01]\n",
    "for idx in alpha_list:\n",
    "    temp = batch_grad_descent(X_train,y_train,alpha=idx, num_iter=1000, check_gradient=True)\n",
    "    if isinstance(temp, tuple):\n",
    "        legend_list.append(idx)\n",
    "        theta_h, loss_h = batch_grad_descent(X_train,y_train,alpha=idx, num_iter=1000, check_gradient=True)\n",
    "        plot_mat.append(loss_h)\n",
    "for i in range(len(plot_mat)):\n",
    "    plt.plot(plot_mat[i])\n",
    "plt.xlabel(\"Instances\")\n",
    "plt.ylabel('Squared Loss')\n",
    "plt.legend(legend_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "###Q2.4b: Implement backtracking line search in batch_gradient_descent\n",
    "###Check http://en.wikipedia.org/wiki/Backtracking_line_search for details\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'theta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7da0ef93f7c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_square_loss_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'theta' is not defined"
     ]
    }
   ],
   "source": [
    "compute_square_loss_gradient(X,y,theta).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################\n",
    "### Compute the gradient of Regularized Batch Gradient Descent\n",
    "def compute_regularized_square_loss_gradient(X, y, theta, lambda_reg=1):\n",
    "    \"\"\"\n",
    "    Compute the gradient of L2-regularized square loss function given X, y and theta\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "        lambda_reg - the regularization coefficient\n",
    "\n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size (num_features)\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    m = X.shape[0]\n",
    "    return 2/m*(-np.matmul(X.transpose(), y) + np.matmul(X.transpose(), np.matmul(X,theta)))+2*lambda_reg*theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regularized_grad_descent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-eb147cab9159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mregularized_grad_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'regularized_grad_descent' is not defined"
     ]
    }
   ],
   "source": [
    " regularized_grad_descent(X_train, y_train, alpha= 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################\n",
    "### Batch Gradient Descent with regularization term\n",
    "def regularized_grad_descent(X, y, alpha=0.1, lambda_reg=1, num_iter=1000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        lambda_reg - the regularization coefficient\n",
    "        numIter - number of iterations to run\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 2D numpy array of size (num_iter+1, num_features)\n",
    "        loss_hist - the history of loss function without the regularization term, 1D numpy array.\n",
    "    \"\"\"\n",
    "    (num_instances, num_features) = X.shape\n",
    "    theta = np.zeros(num_features) #Initialize theta\n",
    "    theta_hist = np.zeros((num_iter+1, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_iter+1) #Initialize loss_hist\n",
    "    #TODO\n",
    "    theta_hist[0] = theta\n",
    "    loss_hist[0] = compute_square_loss(X,y,theta)\n",
    "    for i in range(num_iter):\n",
    "        grad = compute_regularized_square_loss_gradient(X, y, theta_hist[i], lambda_reg)\n",
    "        theta_hist[i+1] = theta_hist[i]-alpha*grad\n",
    "        loss = compute_square_loss(X,y,theta_hist[i+1])\n",
    "        loss_hist[i+1] = loss\n",
    "    return (theta_hist, loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Visualization of Regularized Batch Gradient Descent\n",
    "##X-axis: log(lambda_reg)\n",
    "##Y-axis: square_loss\n",
    "lambda_list1 = [-7, -5, -3,-2, -1, 0, 1, 2]\n",
    "def plot_regularized_batch(X_tr, y_tr, X_te, y_te, lambda_list):\n",
    "    train_loss_final_list = []\n",
    "    test_loss_final_list = []\n",
    "    for i in lambda_list:\n",
    "        train_theta, train_loss = regularized_grad_descent(X_tr, y_tr, alpha = 0.025, lambda_reg=10**(i))\n",
    "        theta = train_theta[-1]\n",
    "        train_loss_final_list.append(compute_square_loss(X_tr,y_tr,theta))\n",
    "        test_loss_final_list.append(compute_square_loss(X_te,y_te,theta))\n",
    "    \n",
    "    plt.plot(lambda_list, train_loss_final_list, label = 'Train')\n",
    "    plt.plot(lambda_list, test_loss_final_list, label = 'Test')\n",
    "    plt.xlabel = 'log(lambda_reg)'\n",
    "    plt.ylabel = 'Square Loss'\n",
    "    plt.legend()\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regularized_batch(X_train, y_train, X_test, y_test, lambda_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambda_list2 = [-3, -2.8, -2.6, -2.4, -2.2, -2, -1.8, -1.6, -1.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regularized_batch(X_train, y_train, X_test, y_test, lambda_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambda_list3 = [-2, -1.95, -1.9, -1.85, -1.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regularized_batch(X_train, y_train, X_test, y_test, lambda_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sgd_loss(X, y, theta, lambda_reg):\n",
    "    #print(len(X), len(y), len(theta))\n",
    "    return 1/len(X)*((np.dot(theta,X)-y)**2 + lambda_reg*np.dot(theta,theta))\n",
    "def compute_sgd_gradient(X, y, theta, lambda_reg):\n",
    "    #return 2/len(X)*((np.dot(theta, X)-y)*X) + 2 * lambda_reg*theta\n",
    "    return (np.dot(theta, X)-y)*X + 2*lambda_reg*theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "### Stochastic Gradient Descent\n",
    "def stochastic_grad_descent(X, y, alpha=0.1, lambda_reg=1, num_iter=1000):\n",
    "    \"\"\"\n",
    "    In this question you will implement stochastic gradient descent with a regularization term\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - string or float. step size in gradient descent\n",
    "                NOTE: In SGD, it's not always a good idea to use a  fixed step size. Usually it's set to 1/sqrt(t) or 1/t\n",
    "                if alpha is a float, then the step size in every iteration is alpha.\n",
    "                if alpha == \"1/sqrt(t)\", alpha = 1/sqrt(t)\n",
    "                if alpha == \"1/t\", alpha = 1/t\n",
    "        lambda_reg - the regularization coefficient\n",
    "        num_iter - number of epochs (i.e number of times) to go through the whole training set\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 3D numpy array of size (num_iter, num_instances, num_features)\n",
    "        loss hist - the history of regularized loss function vector, 2D numpy array of size(num_iter, num_instances)\n",
    "    \"\"\"\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta = np.ones(num_features) #Initialize theta\n",
    "\n",
    "\n",
    "    theta_hist = np.zeros((num_iter, num_instances, num_features))  #Initialize theta_hist\n",
    "    loss_hist = np.zeros((num_iter, num_instances)) #Initialize loss_hist\n",
    "    #TODO\n",
    "#     grad0 = compute_square_loss_gradient(X[row], y, theta_hist[i][row]) + 2*lambda_reg*theta_hist[i][row]\n",
    "    step_size = 1\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        change_idx = np.arange(num_instances)\n",
    "        np.random.shuffle(change_idx)\n",
    "        for row in change_idx:\n",
    "            theta_hist[i][row] = theta\n",
    "            loss_hist[i][row] = compute_sgd_loss(X[row], y[row], theta, lambda_reg)\n",
    "            grad = compute_sgd_gradient(X[row], y[row], theta, lambda_reg)\n",
    "            if isinstance(alpha, float):\n",
    "                theta = theta - alpha*grad\n",
    "            elif alpha == '1/sqrt(t)':\n",
    "                theta = theta - 1./np.sqrt(step_size)*grad\n",
    "                step_size += 1\n",
    "            else:\n",
    "                theta = theta - 1./step_size*grad\n",
    "                step_size += 1\n",
    "    return (theta_hist, loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ex1 = np.arange(10).reshape(2,5)\n",
    "ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2 = np.arange(11,13).reshape(2,1)\n",
    "ex2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.hstack((ex1, ex2))\n",
    "c = np.random.permutation(c)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = c[:,:-1]\n",
    "c[:,-1].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq1 = np.arange(1,4)\n",
    "qq2 = np.arange(4,7)\n",
    "np.dot(qq1, qq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(200).reshape(100, 2)\n",
    "y = np.arange(100)\n",
    "new_arr = np.hstack((X,y[:,np.newaxis]))\n",
    "new_arr = np.random.permutation(new_arr)\n",
    "X_new = new_arr[:,:-1]\n",
    "y_new =  new_arr[:,-1]\n",
    "print(len(X_new[0]))\n",
    "print(y_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_plot(X, y, alpha=0.1, lambda_reg=1, num_iter=1000):\n",
    "    theta_h, loss_h = stochastic_grad_descent(X_train, y_train, alpha, lambda_reg, num_iter)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(np.log(loss_h))\n",
    "    if isinstance(alpha, float):\n",
    "        plt.title('Step size: %1.3f' %alpha)\n",
    "    else:\n",
    "        plt.title('Step size' + alpha)\n",
    "    plt.xlabel = \"Step Number\"\n",
    "    plt.ylabel = \"log(objective function value)\"\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SGD_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-136c2610213a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstep_size_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstep_size_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mSGD_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_reg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_reg_cal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'SGD_plot' is not defined"
     ]
    }
   ],
   "source": [
    "lambda_reg_cal = pow(10,-1.9)\n",
    "step_size_list = [0.05, 0.005]\n",
    "for step in step_size_list:\n",
    "    SGD_plot(X_test, y_test, alpha = step, lambda_reg=lambda_reg_cal, num_iter = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(1750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsMAAAHVCAYAAAAU6/ZZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSWosw7AkSZIayzAsSZKkxvov5HJlsvVkyIgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2618a400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "theta_train, loss_train = stochastic_grad_descent(X_train, y_train, alpha=0.005, lambda_reg=lambda_reg_cal, num_iter = 1000)\n",
    "#theta_test, loss_test = stochastic_grad_descent(X_test, y_test, alpha=0.05, lambda_reg=lambda_reg_cal)\n",
    "num_iteration = 1000\n",
    "num_ins = X_test.shape[0]\n",
    "loss_test = np.zeros((num_iteration, num_ins))\n",
    "for idx in range(num_iteration):\n",
    "    for m in range(num_ins):\n",
    "        loss_test[idx] = loss_test[idx] + compute_sgd_loss(X_test[m], y_test[m], theta_train[idx][m], lambda_reg=lambda_reg_cal)\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(loss_test)\n",
    "#plt.plot(loss_test, c = 'r')\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(100)\n",
    "np.random.shuffle(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "### Visualization that compares the convergence speed of batch\n",
    "###and stochastic gradient descent for various approaches to step_size\n",
    "##X-axis: Step number (for gradient descent) or Epoch (for SGD)\n",
    "##Y-axis: log(objective_function_value) and/or objective_function_value\n",
    "lambda_reg_cal = pow(10,-1.9)\n",
    "step_size_list = [0.05, 0.005]\n",
    "for step in step_size_list:\n",
    "    theta_h, loss_h = stochastic_grad_descent(X_train, y_train, alpha=step, lambda_reg = lambda_reg_cal)\n",
    "    plt.plot(loss_h)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Loading the dataset\n",
    "    print('loading the dataset')\n",
    "\n",
    "    df = pd.read_csv('data.csv', delimiter=',')\n",
    "    X = df.values[:,:-1]\n",
    "    y = df.values[:,-1]\n",
    "\n",
    "    print('Split into Train and Test')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =100, random_state=10)\n",
    "\n",
    "    print(\"Scaling all to [0, 1]\")\n",
    "    X_train, X_test = feature_normalization(X_train, X_test)\n",
    "    X_train = np.hstack((X_train, np.ones((X_train.shape[0], 1))))  # Add bias term\n",
    "    X_test = np.hstack((X_test, np.ones((X_test.shape[0], 1)))) # Add bias term\n",
    "\n",
    "    # TODO\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
